{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example of gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.09842782  2.36980251 -1.71311115 -1.69784127]\n",
      " [-1.10818927 -0.89843516  1.18975116  1.46538825]\n",
      " [ 0.37443655  0.19727268 -0.30769843 -0.39242183]]\n",
      "[[-1.09842782  2.36980251 -1.71311115 -1.69784127]\n",
      " [-1.10818927 -0.89843516  1.18975116  1.46538825]\n",
      " [ 0.37443655  0.19727268 -0.30769843 -0.39242183]]\n"
     ]
    }
   ],
   "source": [
    "# gradient checking: compare the analytical gradient with the numerical gradient\n",
    "# taking the affine layer as an example\n",
    "from gradient_check import eval_numerical_gradient_array\n",
    "import numpy as np\n",
    "from layers import *\n",
    "N = 2\n",
    "D = 3\n",
    "M = 4\n",
    "x = np.random.normal(size=(N, D))\n",
    "w = np.random.normal(size=(D, M))\n",
    "b = np.random.normal(size=(M, ))\n",
    "dout = np.random.normal(size=(N, M))\n",
    "\n",
    "# do a forward pass first\n",
    "out, cache = affine_forward(x,w,b)\n",
    "# check grad f/grad w, the [0] below gets the output out of the (output, cache) original output\n",
    "f=lambda w: affine_forward(x,w,b)[0]\n",
    "# compute the analytical gradient you wrote, [1] get the dw out of the (dx, dw, db) original output\n",
    "grad = affine_backward(dout, cache)[1]\n",
    "# compute the numerical gradient using the provided utility function\n",
    "ngrad = eval_numerical_gradient_array(f, w, dout)\n",
    "print(grad)\n",
    "print(ngrad)\n",
    "# they should be similar enough within some small error tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example of training a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(X):\n",
    "    Xn = np.zeros(X.shape)\n",
    "    for i in range(X.shape[0]):\n",
    "        x = X[i, :]\n",
    "        Xn[i, :] = (x- np.min(x)/(np.max(x)-np.min(x)))\n",
    "    return Xn\n",
    "\n",
    "# def normalization(X):\n",
    "#     Xn = np.zeros(X.shape)\n",
    "#     for i in range(X.shape[0]):\n",
    "#         x = X[i,:]\n",
    "#         Xn[i,:] = (x/(np.sqrt(x.dot(x))+1e-15))\n",
    "#     return Xn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "data = scipy.io.loadmat(\"mnist_data.mat\")\n",
    "X = data['training_data']\n",
    "y = data['training_labels'].ravel()\n",
    "X_test = data['test_data']\n",
    "# X = normalization(X)\n",
    "# X_test = normalization(X_test)\n",
    "\n",
    "# Split the data into a training set and validation set.\n",
    "num_train = X.shape[0]\n",
    "indices = np.array(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[0:50000], indices[50000:]\n",
    "X_train, X_val = X[train_indices], X[val_indices]\n",
    "y_train, y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "# one hot encoding\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "y_train_en = le.fit_transform(y_train)\n",
    "y_val_en = le.fit_transform(y_val)\n",
    "y_train_en[y_train_en == 0] = 0.01\n",
    "y_train_en[y_train_en == 1] = 0.99\n",
    "y_val_en[y_val_en == 0] = 0.01\n",
    "y_val_en[y_val_en == 1] = 0.99\n",
    "\n",
    "# Feature Normalization \n",
    "X_train = X_train.astype('float32'); X_val= X_val.astype('float32'); X_test = X_test.astype('float32')\n",
    "X_train /= 255; X_val /= 255; X_test /= 255\n",
    "\n",
    "from solver import Solver\n",
    "from classifiers.fc_net import FullyConnectedNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {\n",
    "      'X_train': X_train,\n",
    "      'y_train': y_train_en,\n",
    "      'X_val': X_val,\n",
    "      'y_val': y_val_en, \n",
    "    'X_test': X_test} # change to X_train to check predict function\n",
    "\n",
    "\n",
    "# TODO: fill out the hyperparamets\n",
    "hyperparams = {'lr_decay': 1, # 1 is the best\n",
    "               'num_epochs': 30, # usually 10, 100, 500, 1000, and larger. 10-20 is better\n",
    "               'batch_size': 90, # usually 32, 64, and 128  120 is good\n",
    "               'learning_rate':1e-3 # 1e-3 is the best\n",
    "              }\n",
    "\n",
    "# TODO: fill out the number of units in your hidden layers\n",
    "hidden_dim = [700,500,350,200] # this should be a list of units for each hiddent layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 19980) loss: 494.270804\n",
      "(Epoch 0 / 30) train acc: 0.091000; val_acc: 0.102900\n",
      "(Iteration 101 / 19980) loss: 32.322357\n",
      "(Iteration 201 / 19980) loss: 31.752244\n",
      "(Iteration 301 / 19980) loss: 12.892911\n",
      "(Iteration 401 / 19980) loss: 13.884229\n",
      "(Iteration 501 / 19980) loss: 19.979101\n",
      "(Iteration 601 / 19980) loss: 11.992356\n",
      "(Epoch 1 / 30) train acc: 0.967000; val_acc: 0.966300\n",
      "(Iteration 701 / 19980) loss: 5.149894\n",
      "(Iteration 801 / 19980) loss: 7.292631\n",
      "(Iteration 901 / 19980) loss: 9.208693\n",
      "(Iteration 1001 / 19980) loss: 4.958672\n",
      "(Iteration 1101 / 19980) loss: 7.288830\n",
      "(Iteration 1201 / 19980) loss: 8.211078\n",
      "(Iteration 1301 / 19980) loss: 10.748854\n",
      "(Epoch 2 / 30) train acc: 0.990000; val_acc: 0.982200\n",
      "(Iteration 1401 / 19980) loss: 9.932001\n",
      "(Iteration 1501 / 19980) loss: 3.435816\n",
      "(Iteration 1601 / 19980) loss: 9.617993\n",
      "(Iteration 1701 / 19980) loss: 4.775626\n",
      "(Iteration 1801 / 19980) loss: 1.420054\n",
      "(Iteration 1901 / 19980) loss: 5.179353\n",
      "(Epoch 3 / 30) train acc: 0.987000; val_acc: 0.988600\n",
      "(Iteration 2001 / 19980) loss: 1.055832\n",
      "(Iteration 2101 / 19980) loss: 6.444208\n",
      "(Iteration 2201 / 19980) loss: 2.807803\n",
      "(Iteration 2301 / 19980) loss: 4.053298\n",
      "(Iteration 2401 / 19980) loss: 0.345379\n",
      "(Iteration 2501 / 19980) loss: 0.468483\n",
      "(Iteration 2601 / 19980) loss: 8.009607\n",
      "(Epoch 4 / 30) train acc: 0.992000; val_acc: 0.992100\n",
      "(Iteration 2701 / 19980) loss: 7.085673\n",
      "(Iteration 2801 / 19980) loss: 2.464994\n",
      "(Iteration 2901 / 19980) loss: 1.428481\n",
      "(Iteration 3001 / 19980) loss: 1.751535\n",
      "(Iteration 3101 / 19980) loss: 1.592935\n",
      "(Iteration 3201 / 19980) loss: 2.073222\n",
      "(Iteration 3301 / 19980) loss: 0.210215\n",
      "(Epoch 5 / 30) train acc: 0.991000; val_acc: 0.993300\n",
      "(Iteration 3401 / 19980) loss: 1.412590\n",
      "(Iteration 3501 / 19980) loss: 2.969782\n",
      "(Iteration 3601 / 19980) loss: 1.476360\n",
      "(Iteration 3701 / 19980) loss: 0.213346\n",
      "(Iteration 3801 / 19980) loss: 3.583495\n",
      "(Iteration 3901 / 19980) loss: 2.030453\n",
      "(Epoch 6 / 30) train acc: 0.993000; val_acc: 0.996800\n",
      "(Iteration 4001 / 19980) loss: 0.349632\n",
      "(Iteration 4101 / 19980) loss: 0.110706\n",
      "(Iteration 4201 / 19980) loss: 0.441422\n",
      "(Iteration 4301 / 19980) loss: 3.020271\n",
      "(Iteration 4401 / 19980) loss: 0.760076\n",
      "(Iteration 4501 / 19980) loss: 0.028699\n",
      "(Iteration 4601 / 19980) loss: 0.120541\n",
      "(Epoch 7 / 30) train acc: 0.999000; val_acc: 0.998800\n",
      "(Iteration 4701 / 19980) loss: 0.132483\n",
      "(Iteration 4801 / 19980) loss: 1.062187\n",
      "(Iteration 4901 / 19980) loss: 3.967183\n",
      "(Iteration 5001 / 19980) loss: 0.466945\n",
      "(Iteration 5101 / 19980) loss: 0.203239\n",
      "(Iteration 5201 / 19980) loss: 0.247834\n",
      "(Iteration 5301 / 19980) loss: 1.396690\n",
      "(Epoch 8 / 30) train acc: 1.000000; val_acc: 0.999500\n",
      "(Iteration 5401 / 19980) loss: 0.038367\n",
      "(Iteration 5501 / 19980) loss: 0.679423\n",
      "(Iteration 5601 / 19980) loss: 0.359853\n",
      "(Iteration 5701 / 19980) loss: 0.287904\n",
      "(Iteration 5801 / 19980) loss: 0.034672\n",
      "(Iteration 5901 / 19980) loss: 0.158787\n",
      "(Epoch 9 / 30) train acc: 1.000000; val_acc: 0.999700\n",
      "(Iteration 6001 / 19980) loss: 0.059038\n",
      "(Iteration 6101 / 19980) loss: 0.077096\n",
      "(Iteration 6201 / 19980) loss: 0.101897\n",
      "(Iteration 6301 / 19980) loss: 0.061586\n",
      "(Iteration 6401 / 19980) loss: 0.088259\n",
      "(Iteration 6501 / 19980) loss: 0.093482\n",
      "(Iteration 6601 / 19980) loss: 0.092066\n",
      "(Epoch 10 / 30) train acc: 1.000000; val_acc: 0.999800\n",
      "(Iteration 6701 / 19980) loss: 0.031081\n",
      "(Iteration 6801 / 19980) loss: 0.038372\n",
      "(Iteration 6901 / 19980) loss: 0.037591\n",
      "(Iteration 7001 / 19980) loss: 0.053566\n",
      "(Iteration 7101 / 19980) loss: 0.012983\n",
      "(Iteration 7201 / 19980) loss: 0.043391\n",
      "(Iteration 7301 / 19980) loss: 0.046440\n",
      "(Epoch 11 / 30) train acc: 1.000000; val_acc: 1.000000\n",
      "(Iteration 7401 / 19980) loss: 0.447957\n",
      "(Iteration 7501 / 19980) loss: 0.049053\n",
      "(Iteration 7601 / 19980) loss: 0.042592\n",
      "(Iteration 7701 / 19980) loss: 0.077300\n",
      "(Iteration 7801 / 19980) loss: 0.052878\n",
      "(Iteration 7901 / 19980) loss: 0.035122\n",
      "(Epoch 12 / 30) train acc: 1.000000; val_acc: 1.000000\n",
      "(Iteration 8001 / 19980) loss: 0.009103\n",
      "(Iteration 8101 / 19980) loss: 0.139387\n",
      "(Iteration 8201 / 19980) loss: 0.104648\n",
      "(Iteration 8301 / 19980) loss: 0.049169\n",
      "(Iteration 8401 / 19980) loss: 0.056676\n",
      "(Iteration 8501 / 19980) loss: 0.304327\n",
      "(Iteration 8601 / 19980) loss: 0.091344\n",
      "(Epoch 13 / 30) train acc: 1.000000; val_acc: 1.000000\n",
      "(Iteration 8701 / 19980) loss: 0.067146\n",
      "(Iteration 8801 / 19980) loss: 0.083981\n",
      "(Iteration 8901 / 19980) loss: 0.067744\n",
      "(Iteration 9001 / 19980) loss: 0.064418\n",
      "(Iteration 9101 / 19980) loss: 0.017271\n",
      "(Iteration 9201 / 19980) loss: 0.039728\n",
      "(Iteration 9301 / 19980) loss: 0.021129\n",
      "(Epoch 14 / 30) train acc: 1.000000; val_acc: 1.000000\n",
      "(Iteration 9401 / 19980) loss: 0.016617\n",
      "(Iteration 9501 / 19980) loss: 0.033790\n",
      "(Iteration 9601 / 19980) loss: 0.030728\n",
      "(Iteration 9701 / 19980) loss: 0.026997\n",
      "(Iteration 9801 / 19980) loss: 0.017796\n",
      "(Iteration 9901 / 19980) loss: 0.074510\n",
      "(Epoch 15 / 30) train acc: 1.000000; val_acc: 1.000000\n",
      "(Iteration 10001 / 19980) loss: 0.045140\n",
      "(Iteration 10101 / 19980) loss: 0.027934\n",
      "(Iteration 10201 / 19980) loss: 0.033519\n",
      "(Iteration 10301 / 19980) loss: 0.050966\n",
      "(Iteration 10401 / 19980) loss: 0.002933\n",
      "(Iteration 10501 / 19980) loss: 0.053811\n",
      "(Iteration 10601 / 19980) loss: 0.004114\n",
      "(Epoch 16 / 30) train acc: 1.000000; val_acc: 1.000000\n",
      "(Iteration 10701 / 19980) loss: 0.048979\n",
      "(Iteration 10801 / 19980) loss: 0.029325\n",
      "(Iteration 10901 / 19980) loss: 0.014853\n",
      "(Iteration 11001 / 19980) loss: 0.023196\n",
      "(Iteration 11101 / 19980) loss: 0.046854\n",
      "(Iteration 11201 / 19980) loss: 0.010504\n",
      "(Iteration 11301 / 19980) loss: 0.035493\n",
      "(Epoch 17 / 30) train acc: 1.000000; val_acc: 1.000000\n",
      "(Iteration 11401 / 19980) loss: 0.075931\n",
      "(Iteration 11501 / 19980) loss: 0.040315\n",
      "(Iteration 11601 / 19980) loss: 0.014615\n",
      "(Iteration 11701 / 19980) loss: 0.015472\n",
      "(Iteration 11801 / 19980) loss: 0.014995\n",
      "(Iteration 11901 / 19980) loss: 0.010274\n",
      "(Epoch 18 / 30) train acc: 1.000000; val_acc: 1.000000\n",
      "(Iteration 12001 / 19980) loss: 0.022483\n",
      "(Iteration 12101 / 19980) loss: 0.029264\n",
      "(Iteration 12201 / 19980) loss: 0.042381\n",
      "(Iteration 12301 / 19980) loss: 0.029367\n",
      "(Iteration 12401 / 19980) loss: 0.028753\n",
      "(Iteration 12501 / 19980) loss: 0.009049\n",
      "(Iteration 12601 / 19980) loss: 0.008119\n",
      "(Epoch 19 / 30) train acc: 1.000000; val_acc: 1.000000\n",
      "(Iteration 12701 / 19980) loss: 0.014157\n",
      "(Iteration 12801 / 19980) loss: 0.031911\n",
      "(Iteration 12901 / 19980) loss: 0.011990\n",
      "(Iteration 13001 / 19980) loss: 0.006171\n",
      "(Iteration 13101 / 19980) loss: 0.017102\n",
      "(Iteration 13201 / 19980) loss: 0.009833\n",
      "(Iteration 13301 / 19980) loss: 0.020518\n",
      "(Epoch 20 / 30) train acc: 1.000000; val_acc: 1.000000\n",
      "(Iteration 13401 / 19980) loss: 0.006586\n",
      "(Iteration 13501 / 19980) loss: 0.011984\n",
      "(Iteration 13601 / 19980) loss: 0.023670\n",
      "(Iteration 13701 / 19980) loss: 0.003924\n",
      "(Iteration 13801 / 19980) loss: 0.033030\n",
      "(Iteration 13901 / 19980) loss: 0.016578\n",
      "(Epoch 21 / 30) train acc: 1.000000; val_acc: 1.000000\n",
      "(Iteration 14001 / 19980) loss: 0.009654\n",
      "(Iteration 14101 / 19980) loss: 0.014666\n",
      "(Iteration 14201 / 19980) loss: 0.008036\n",
      "(Iteration 14301 / 19980) loss: 0.005947\n",
      "(Iteration 14401 / 19980) loss: 0.003858\n",
      "(Iteration 14501 / 19980) loss: 0.015780\n",
      "(Iteration 14601 / 19980) loss: 0.010962\n",
      "(Epoch 22 / 30) train acc: 1.000000; val_acc: 1.000000\n",
      "(Iteration 14701 / 19980) loss: 0.003903\n",
      "(Iteration 14801 / 19980) loss: 0.009879\n",
      "(Iteration 14901 / 19980) loss: 0.006045\n",
      "(Iteration 15001 / 19980) loss: 0.013381\n",
      "(Iteration 15101 / 19980) loss: 0.012587\n",
      "(Iteration 15201 / 19980) loss: 0.008150\n",
      "(Iteration 15301 / 19980) loss: 0.012222\n",
      "(Epoch 23 / 30) train acc: 1.000000; val_acc: 1.000000\n",
      "(Iteration 15401 / 19980) loss: 0.009672\n",
      "(Iteration 15501 / 19980) loss: 0.001229\n",
      "(Iteration 15601 / 19980) loss: 0.003886\n",
      "(Iteration 15701 / 19980) loss: 0.016314\n",
      "(Iteration 15801 / 19980) loss: 0.003193\n",
      "(Iteration 15901 / 19980) loss: 0.014686\n",
      "(Epoch 24 / 30) train acc: 1.000000; val_acc: 1.000000\n",
      "(Iteration 16001 / 19980) loss: 0.001313\n",
      "(Iteration 16101 / 19980) loss: 0.005389\n",
      "(Iteration 16201 / 19980) loss: 0.034258\n",
      "(Iteration 16301 / 19980) loss: 0.010667\n",
      "(Iteration 16401 / 19980) loss: 0.020555\n",
      "(Iteration 16501 / 19980) loss: 0.016326\n",
      "(Iteration 16601 / 19980) loss: 0.018641\n",
      "(Epoch 25 / 30) train acc: 1.000000; val_acc: 1.000000\n",
      "(Iteration 16701 / 19980) loss: 0.021038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 16801 / 19980) loss: 0.029400\n",
      "(Iteration 16901 / 19980) loss: 0.018437\n",
      "(Iteration 17001 / 19980) loss: 0.009872\n",
      "(Iteration 17101 / 19980) loss: 0.015708\n",
      "(Iteration 17201 / 19980) loss: 0.018686\n",
      "(Iteration 17301 / 19980) loss: 0.005891\n",
      "(Epoch 26 / 30) train acc: 1.000000; val_acc: 1.000000\n",
      "(Iteration 17401 / 19980) loss: 0.008886\n",
      "(Iteration 17501 / 19980) loss: 0.016581\n",
      "(Iteration 17601 / 19980) loss: 0.005945\n",
      "(Iteration 17701 / 19980) loss: 0.013497\n",
      "(Iteration 17801 / 19980) loss: 0.010685\n",
      "(Iteration 17901 / 19980) loss: 0.000970\n",
      "(Epoch 27 / 30) train acc: 1.000000; val_acc: 1.000000\n",
      "(Iteration 18001 / 19980) loss: 0.016987\n",
      "(Iteration 18101 / 19980) loss: 0.011529\n",
      "(Iteration 18201 / 19980) loss: 0.014291\n",
      "(Iteration 18301 / 19980) loss: 0.004119\n",
      "(Iteration 18401 / 19980) loss: 0.012931\n",
      "(Iteration 18501 / 19980) loss: 0.002196\n",
      "(Iteration 18601 / 19980) loss: 0.003573\n",
      "(Epoch 28 / 30) train acc: 1.000000; val_acc: 1.000000\n",
      "(Iteration 18701 / 19980) loss: 0.001582\n",
      "(Iteration 18801 / 19980) loss: 0.007284\n",
      "(Iteration 18901 / 19980) loss: 0.002026\n",
      "(Iteration 19001 / 19980) loss: 0.006062\n",
      "(Iteration 19101 / 19980) loss: 0.002172\n",
      "(Iteration 19201 / 19980) loss: 0.004349\n",
      "(Iteration 19301 / 19980) loss: 0.004407\n",
      "(Epoch 29 / 30) train acc: 1.000000; val_acc: 1.000000\n",
      "(Iteration 19401 / 19980) loss: 0.007117\n",
      "(Iteration 19501 / 19980) loss: 0.011051\n",
      "(Iteration 19601 / 19980) loss: 0.010798\n",
      "(Iteration 19701 / 19980) loss: 0.004024\n",
      "(Iteration 19801 / 19980) loss: 0.005373\n",
      "(Iteration 19901 / 19980) loss: 0.004544\n",
      "(Epoch 30 / 30) train acc: 1.000000; val_acc: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# hyperparams = {'lr_decay': 1, # 1 is the best\n",
    "#                'num_epochs': 20, # usually 10, 100, 500, 1000, and larger. 10-20 is better\n",
    "#                'batch_size': 100, # usually 32, 64, and 128 \n",
    "#                'learning_rate':1e-3 # 1e-3 is the best\n",
    "#               }\n",
    "\n",
    "model = FullyConnectedNet(input_dim=784,\n",
    "                          hidden_dim=hidden_dim)\n",
    "solver = Solver(model, data,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': hyperparams['learning_rate'],\n",
    "                },\n",
    "                lr_decay=hyperparams['lr_decay'],\n",
    "                num_epochs=hyperparams['num_epochs'], \n",
    "                batch_size=hyperparams['batch_size'],\n",
    "                print_every=100)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 784))"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict X_test\n",
    "y_pred = solver.predict()\n",
    "y_pred.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 0, ..., 4, 5, 6], dtype=int64)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def results_to_csv(y_test):\n",
    "    y_test = y_test.astype(int)\n",
    "    df = pd.DataFrame({'Category': y_test})\n",
    "    df.index += 1  # Ensures that the index starts at 1. \n",
    "    df.to_csv('submission_m_8.8.15.57.csv', index_label='Id')\n",
    "results_to_csv(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check my predict function\n",
    "y_pred_X_train = solver.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dizhe\\Anaconda3\\envs\\mcm\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seems my predict fn is correct\n",
    "np.sum(y_pred_X_train == y_train)/len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cache1': 1, 'cache3': 3}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdict = {}\n",
    "newdict['cache%d'%1] = 1 \n",
    "newdict['cache%d' %(1+2)] = 3\n",
    "newdict"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
