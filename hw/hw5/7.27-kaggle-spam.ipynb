{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import scipy.io \n",
    "from scipy import stats\n",
    "import random\n",
    "import pandas as pd\n",
    "from statistics import mode\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(X):\n",
    "    Xn = np.zeros(X.shape)\n",
    "    for i in range(X.shape[0]):\n",
    "        x = X[i, :]\n",
    "        Xn[i, :] = (x- np.min(x)/(np.max(x)-np.min(x)))\n",
    "    return Xn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"Ham\", \"Spam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \n",
    "    class Node:\n",
    "        def __init__(self, split_rule, left, right, label, is_leaf):\n",
    "            self.split_rule = split_rule\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.label = label\n",
    "            self.is_leaf = is_leaf # 1 = stop \n",
    "            \n",
    "        def __repr__(self):\n",
    "            \"\"\"\n",
    "            TODO: one way to visualize the decision tree is to write out a __repr__ method\n",
    "            that returns the string representation of a tree. Think about how to visualize\n",
    "            a tree structure. You might have seen this before in CS61A.\n",
    "            \"\"\"\n",
    "            def viz(Node, prefix, symbol):\n",
    "                if not Node:\n",
    "                    return prefix + '[]'\n",
    "                if Node.is_leaf == 1:\n",
    "                    return(prefix + '(Therefore the email was: '+ class_names[Node.label] + ')')\n",
    "                else:\n",
    "                    ret = (prefix + '[Feature: '+ str(Node.split_rule[0]) + \n",
    "                           ', Threshold: '+symbol+ str(Node.split_rule[1]) + ']')\n",
    "                    ret += '\\n' + viz(Node.left, prefix + '\\t','<=') + '\\n' + viz(Node.right, prefix + '\\t','>')\n",
    "                    return ret\n",
    "                \n",
    "            return viz(self, \"\",'<=')\n",
    "    \n",
    "        \n",
    "    def __init__(self, max_depth = 200):\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def max_count(self, array):\n",
    "        return stats.mode(array, nan_policy='omit')[0][0]\n",
    "    \n",
    "    def entropy(self,y):\n",
    "        p = y / (np.sum(y)+1e-10)\n",
    "        return -p.dot(np.log2(p+1e-10))\n",
    "    \n",
    "    def entropy_impurity(self,left_y_freq, right_y_freq):\n",
    "        Sl = np.sum(left_y_freq)\n",
    "        Sr = np.sum(right_y_freq)\n",
    "        return (Sl * self.entropy(left_y_freq) + Sr * self.entropy(right_y_freq)) / (Sl+Sr)\n",
    "    \n",
    "    def information_gain(self,left_y_freq, right_y_freq):\n",
    "        total = left_y_freq + right_y_freq\n",
    "        if self.entropy(total) == 0: # see if it is pure\n",
    "            return -1\n",
    "        else:\n",
    "            infor_gain = self.entropy(total) - self.entropy_impurity(left_y_freq, right_y_freq)\n",
    "        return infor_gain\n",
    "    \n",
    "#     def gini(y): \n",
    "#         p = y / (np.sum(y)+1e-20)\n",
    "#         gini = 1-np.sum(p**2)\n",
    "#         return gini\n",
    "\n",
    "#     def gini_impurity(left_label_freq, right_label_freq): # useless\n",
    "#         Sl = np.sum(left_label_freq)\n",
    "#         Sr = np.sum(right_label_freq)\n",
    "#         return (Sl * gini(left_label_freq) + Sr * gini(right_label_freq)) / (Sl+Sr)\n",
    "\n",
    "#     def gini_purification(X, y, thresh):\n",
    "#         \"\"\"\n",
    "#         TODO: implement a method that calculates reduction in impurity gain given a vector of features\n",
    "#         and a split threshold\n",
    "#         \"\"\"\n",
    "#         return 0\n",
    "    \n",
    "    def split(self, S, depth, random_f = -1,verbose = False): # recursively\n",
    "        \"\"\"\n",
    "        TODO: implement a method that return a split of the dataset given an index of the feature and\n",
    "        a threshold for it\n",
    "        \"\"\"\n",
    "#         print((depth-1) * '    '+'Depth: '+ str(depth))\n",
    "        if depth >= self.max_depth: \n",
    "#             print('label: '+ str(self.max_count(self.labels[S])))\n",
    "            node = self.Node(left=None, right=None, split_rule=None, is_leaf=1, label=self.max_count(self.labels[S]))\n",
    "            if verbose == True:\n",
    "                print(repr(node))\n",
    "            return node\n",
    "        else:\n",
    "            max_feature, max_thresh = self.segmenter(self.data[S, :], self.labels[S],random_f = random_f)\n",
    "            \n",
    "#             print((depth-1) * '    '+'depth:' + str(depth)  + ', feature index: ' + str(max_feature) +', threshold: ' + str(max_thresh))\n",
    "            Sl = [i for i in S if self.data[i, max_feature] <= max_thresh]\n",
    "            Sr = [i for i in S if self.data[i, max_feature] > max_thresh]\n",
    "#             print((depth-1) * '    '+\"left group: \" + str(len(Sl)) + ', right group: ' + str(len(Sr)))\n",
    "            if len(Sl) <= 5 or len(Sr) <= 5: \n",
    "#                 print('label: '+ str(self.max_count(self.labels[S])))\n",
    "                node = self.Node(left=None, right=None, split_rule=None, is_leaf=1, label=self.max_count(self.labels[S]))\n",
    "                if verbose == True:\n",
    "                    print(repr(node))\n",
    "                return node\n",
    "            else:\n",
    "                node = self.Node(left=self.split(Sl, depth+1, random_f), right=self.split(Sr,depth+1,random_f), split_rule = (max_feature, max_thresh), is_leaf=0, label=None)\n",
    "                if verbose == True:\n",
    "                    print(repr(node))\n",
    "                return node\n",
    "    \n",
    "    \n",
    "    def iter_thresh(self, X, y):\n",
    "        row_f = sorted(set(X)) \n",
    "        col_l = set(y)\n",
    "        freq_matrix = np.zeros([len(row_f), len(col_l)])\n",
    "        for i, j in enumerate(row_f):\n",
    "            for k, l in enumerate(col_l):\n",
    "                freq_matrix[i, k] = len(y[np.where(y[np.where(X==j)]==l)])\n",
    "        all_thresh = np.array(row_f[1:] + row_f[-1:]) / 2.\n",
    "        left_freq = np.zeros([len(col_l)])\n",
    "        right_freq = np.sum(freq_matrix, axis=0)\n",
    "        left_freq_sum = 0\n",
    "        max_thresh = all_thresh[0]\n",
    "        \n",
    "        max_gain = self.information_gain(left_freq, right_freq)\n",
    "        for i, thresh in enumerate(all_thresh):\n",
    "            left_freq += freq_matrix[i, :]\n",
    "            right_freq -= freq_matrix[i, :]\n",
    "            gain = self.information_gain(left_freq, right_freq)\n",
    "            if gain > max_gain:\n",
    "                max_gain = gain\n",
    "                max_thresh = thresh\n",
    "        return max_thresh, max_gain\n",
    "    \n",
    "    \n",
    "    def segmenter(self, X, y, random_f = -1):\n",
    "        \"\"\"\n",
    "        TODO: compute entropy gain for all single-dimension splits,\n",
    "        return the feature and the threshold for the split that\n",
    "        has maximum gain\n",
    "        \"\"\"        \n",
    "        x = X.shape[1]\n",
    "        if random_f == -1:\n",
    "            all_features = np.arange(x)\n",
    "        else:\n",
    "            all_features = np.random.choice(range(x), random_f)\n",
    "            \n",
    "        all_features = np.arange(x)\n",
    "        max_gain = 1e-10\n",
    "        max_thresh = 0\n",
    "        max_feature = 0\n",
    "        for i in all_features:\n",
    "            thresh, gain = self.iter_thresh(X[:, i], y)\n",
    "            if gain > max_gain:\n",
    "                max_gain = gain\n",
    "                max_thresh = thresh\n",
    "                max_feature = i\n",
    "        return max_feature, max_thresh\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, random_f = -1, verbose = False):\n",
    "        \"\"\"\n",
    "        TODO: fit the model to a training set. Think about what would be \n",
    "        your stopping criteria\n",
    "        \"\"\"\n",
    "        self.data = X\n",
    "        self.labels = y\n",
    "        S = np.array(range(len(y)))\n",
    "        self.root = self.split(S, 1 , random_f = random_f, verbose = verbose)\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def predict(self, X, T = 0):\n",
    "        \"\"\"\n",
    "        TODO: predict the labels for input data \n",
    "        \"\"\"\n",
    "        if T == 1: # T = 1, for random forest\n",
    "            X = np.reshape(X, [1, len(X)])\n",
    "            row_num = 1\n",
    "        else:\n",
    "            row_num = X.shape[0]\n",
    "        labels = np.zeros(row_num)    \n",
    "        depth = 0    \n",
    "        for i in range(row_num):  \n",
    "            current_node = self.root\n",
    "            while current_node.is_leaf == 0: \n",
    "                feature = current_node.split_rule[0]\n",
    "                thresh = current_node.split_rule[1]\n",
    "                if X[i,:][feature] <= thresh:\n",
    "                    current_node = current_node.left\n",
    "                else:\n",
    "                    current_node = current_node.right\n",
    "            depth += 1\n",
    "            labels[i] = current_node.label\n",
    "        return labels\n",
    "    \n",
    "    def accuracy(self, X, y_val, T = 0):\n",
    "        y_pred = self.predict(X, T = T)\n",
    "        len_y = float(len(y_pred))\n",
    "        return np.sum(y_pred == y_val) / len_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    \n",
    "    def __init__(self, n_trees=20, n_sample=1000, random_f=-1, max_depth=200):\n",
    "        \"\"\"\n",
    "        TODO: initialization of a random forest\n",
    "        \"\"\"\n",
    "        self.n_trees = n_trees\n",
    "        self.n_sample = n_sample\n",
    "        self.random_f = random_f\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = np.array([DecisionTree(max_depth)] * n_trees)\n",
    "        \n",
    "    def fit(self, X, y, verbose = False):\n",
    "        \"\"\"\n",
    "        TODO: fit the model to a training set.\n",
    "        \"\"\"\n",
    "        if self.random_f == -1:\n",
    "            self.random_f = int(np.log2(X.shape[1]))\n",
    "        results = np.zeros(self.n_trees, dtype=object)\n",
    "        for i, dt in enumerate(self.trees):\n",
    "            print('#%d. tree' % (i+1))\n",
    "            idx = np.random.choice(range(len(X)), self.n_sample)\n",
    "            sub_X = X[idx, :]\n",
    "            sub_y = y[idx]\n",
    "            results[i] = dt.fit(sub_X, sub_y, random_f=self.random_f, verbose = verbose)\n",
    "        self.trees = results\n",
    "\n",
    "    def predict(self, X, T = 1):\n",
    "        \"\"\"\n",
    "        TODO: predict the labels for input data \n",
    "        \"\"\"\n",
    "        row_num = X.shape[0]\n",
    "        labels = []    \n",
    "        for i in range(row_num):\n",
    "            pred = np.zeros(self.n_trees)\n",
    "            for j, dt in enumerate(self.trees):\n",
    "                pred[j] = dt.predict(X[i, :], T = T)\n",
    "            labels.append(dt.max_count(pred))\n",
    "        return labels\n",
    "    \n",
    "    def accuracy(self, X, y_val, T = 1):\n",
    "        y_pred = self.predict(X,T = T)\n",
    "        N = float(len(y_pred))\n",
    "        return np.sum(y_pred == y_val) / N\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spam data\n",
    "spam = scipy.io.loadmat('datasets/spam-dataset/spam_data.mat')\n",
    "train_X = spam['training_data']\n",
    "train_y = spam['training_labels'].ravel()\n",
    "test_X = spam['test_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "idx = np.random.choice(range(len(train_y)), int(len(train_y)), replace=False)\n",
    "train_size = int(len(train_X)*0.8)\n",
    "X_train = train_X[idx,:][:train_size,:]\n",
    "y_train = train_y[idx][:train_size]\n",
    "X_val = train_X[idx,:][train_size:,:]\n",
    "y_val = train_y[idx][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feature: 28, Threshold: <=0.5]\n",
      "\t[Feature: 19, Threshold: <=0.5]\n",
      "\t\t[Feature: 29, Threshold: <=0.5]\n",
      "\t\t\t[Feature: 16, Threshold: <=0.5]\n",
      "\t\t\t\t[Feature: 0, Threshold: <=0.5]\n",
      "\t\t\t\t\t[Feature: 31, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t[Feature: 25, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t[Feature: 30, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t[Feature: 6, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t\t[Feature: 13, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t\t\t[Feature: 20, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t\t\t[Feature: 13, Threshold: >1.0]\n",
      "\t\t\t\t\t\t\t\t\t\t\t[Feature: 26, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t\t\t[Feature: 26, Threshold: >0.5]\n",
      "\t\t\t\t\t\t\t\t[Feature: 25, Threshold: <=3.5]\n",
      "\t\t\t\t\t\t\t\t\t[Feature: 24, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t\t\t[Feature: 15, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t\t[Feature: 15, Threshold: >0.5]\n",
      "\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t[Feature: 26, Threshold: >0.5]\n",
      "\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t[Feature: 1, Threshold: >0.5]\n",
      "\t\t\t\t\t[Feature: 30, Threshold: <=2.0]\n",
      "\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t(Therefore the email was: Ham)\n",
      "\t[Feature: 19, Threshold: >0.5]\n",
      "\t\t[Feature: 31, Threshold: <=0.5]\n",
      "\t\t\t[Feature: 3, Threshold: <=0.5]\n",
      "\t\t\t\t[Feature: 26, Threshold: <=1.0]\n",
      "\t\t\t\t\t[Feature: 6, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t[Feature: 16, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t[Feature: 15, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t[Feature: 25, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t\t[Feature: 0, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t\t\t[Feature: 29, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t\t\t\t[Feature: 27, Threshold: <=1.0]\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t\t\t\t\t[Feature: 27, Threshold: >0.5]\n",
      "\t\t\t\t\t\t\t\t\t\t[Feature: 9, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t\t\t\t[Feature: 25, Threshold: >5.0]\n",
      "\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t[Feature: 15, Threshold: >2.5]\n",
      "\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t\t[Feature: 5, Threshold: >0.5]\n",
      "\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t[Feature: 28, Threshold: >2.0]\n",
      "\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t[Feature: 29, Threshold: >1.0]\n",
      "\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t[Feature: 3, Threshold: >0.5]\n",
      "\t\t\t\t[Feature: 15, Threshold: <=0.5]\n",
      "\t\t\t\t\t[Feature: 12, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t(Therefore the email was: Ham)\n",
      "Training accuracy:  0.8271694464587865\n",
      "Validation accuracy:  0.8154589371980676\n"
     ]
    }
   ],
   "source": [
    "# try training\n",
    "dt = DecisionTree(15)\n",
    "dt.fit(X_train, y_train, verbose = True)\n",
    "train_acc = dt.accuracy(X_train, y_train)\n",
    "print('Training accuracy: ', train_acc)\n",
    "val_acc = dt.accuracy(X_val, y_val)\n",
    "print('Validation accuracy: ', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dt.predict(test_X)\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "def results_to_csv(y_test):\n",
    "    y_test = y_test.astype(int)\n",
    "    df = pd.DataFrame({'Category': y_test})\n",
    "    df.index += 1  # Ensures that the index starts at 1. \n",
    "    df.to_csv('submission_spam_low2.csv', index_label='Id')\n",
    "    \n",
    "results_to_csv(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import glob\n",
    "\n",
    "BASE_DIR = 'datasets/spam-dataset/'\n",
    "SPAM_DIR = 'spam/'\n",
    "HAM_DIR = 'ham/'\n",
    "TEST_DIR = 'test/'\n",
    "\n",
    "NUM_TRAINING_EXAMPLES = 5172\n",
    "NUM_TEST_EXAMPLES = 5857\n",
    "\n",
    "spam_filenames = glob.glob(BASE_DIR + SPAM_DIR + '*.txt')\n",
    "ham_filenames = glob.glob(BASE_DIR + HAM_DIR + '*.txt')\n",
    "test_filenames = [BASE_DIR + TEST_DIR + str(x) + '.txt' for x in range(NUM_TEST_EXAMPLES)]\n",
    "\n",
    "train_text = []\n",
    "for file in spam_filenames+ham_filenames: \n",
    "    with open(file, \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "        train_text.append(f.read())\n",
    "        \n",
    "test_text = []\n",
    "for file in test_filenames: \n",
    "    with open(file, \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "        test_text.append(f.read())\n",
    "        \n",
    "vectorizer = CountVectorizer()\n",
    "train_X = normalization(vectorizer.fit_transform(train_text).toarray())\n",
    "test_X = normalization(vectorizer.transform(test_text).toarray())\n",
    "train_y = np.concatenate((np.ones(len(spam_filenames)), np.zeros(len(ham_filenames))))\n",
    "train_y = train_y.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # feature selection\n",
    "# std = np.std(train_X, axis=0)\n",
    "# idx = std.argsort()[-5000:]\n",
    "# train_X_selected = train_X[:, idx]\n",
    "# test_X_selected = test_X[:, idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "best = SelectKBest(score_func=chi2, k=5000)\n",
    "fit = best.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(pd.DataFrame(train_X).columns)\n",
    "\n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  \n",
    "topindex = featureScores['Score'].sort_values(ascending = False)[:5000].index\n",
    "topindex = topindex.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_selected = train_X[:,topindex]\n",
    "test_X_selected = test_X[:,topindex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "idx = np.random.choice(range(len(train_y)),len(train_y),replace = False)\n",
    "split = int(len(train_y)*0.8)\n",
    "X_train,X_val = train_X_selected[idx][:split,],train_X_selected[idx][split:,]\n",
    "y_train,y_val = train_y[idx][:split],train_y[idx][split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feature: 2, Threshold: <=0.5]\n",
      "\t[Feature: 12, Threshold: <=0.5]\n",
      "\t\t[Feature: 19, Threshold: <=0.5]\n",
      "\t\t\t[Feature: 4, Threshold: <=0.5]\n",
      "\t\t\t\t[Feature: 31, Threshold: <=0.5]\n",
      "\t\t\t\t\t[Feature: 11, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t[Feature: 58, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t[Feature: 18, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t[Feature: 17, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t\t[Feature: 70, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t\t\t[Feature: 1664, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t\t\t\t[Feature: 93, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t[Feature: 22, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t[Feature: 9, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t[Feature: 94, Threshold: >0.5]\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t[Feature: 35, Threshold: >1.5]\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t[Feature: 2105, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t[Feature: 39, Threshold: >4.0]\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t\t[Feature: 316, Threshold: >2.5]\n",
      "\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t[Feature: 35, Threshold: >5.0]\n",
      "\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t\t\t[Feature: 7, Threshold: >7.0]\n",
      "\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t[Feature: 48, Threshold: >0.5]\n",
      "\t\t\t\t\t\t[Feature: 41, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t[Feature: 0, Threshold: <=0]\n",
      "\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t[Feature: 39, Threshold: >3.5]\n",
      "\t\t\t\t\t\t\t[Feature: 1662, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t[Feature: 30, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t[Feature: 1007, Threshold: >1.0]\n",
      "\t\t\t\t\t[Feature: 1121, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t[Feature: 1527, Threshold: <=1.0]\n",
      "\t\t\t\t\t\t\t[Feature: 255, Threshold: <=1.0]\n",
      "\t\t\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t\t\t\t[Feature: 396, Threshold: >1.5]\n",
      "\t\t\t\t\t\t\t\t\t[Feature: 30, Threshold: <=0.5]\n",
      "\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t\t\t\t\t\t(Therefore the email was: Spam)\n",
      "\t\t\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t[Feature: 0, Threshold: >0]\n",
      "\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t\t(Therefore the email was: Ham)\n",
      "\t\t[Feature: 0, Threshold: >0]\n",
      "\t\t\t(Therefore the email was: Ham)\n",
      "\t\t\t(Therefore the email was: Ham)\n",
      "\t[Feature: 0, Threshold: >0]\n",
      "\t\t(Therefore the email was: Ham)\n",
      "\t\t(Therefore the email was: Ham)\n",
      "Training accuracy:  0.9395697365240513\n",
      "Validation accuracy:  0.9352657004830918\n"
     ]
    }
   ],
   "source": [
    "# try training\n",
    "dt = DecisionTree(15)\n",
    "dt.fit(X_train, y_train, verbose = True)\n",
    "train_acc = dt.accuracy(X_train, y_train)\n",
    "print('Training accuracy: ', train_acc)\n",
    "val_acc = dt.accuracy(X_val, y_val)\n",
    "print('Validation accuracy: ', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1. tree\n",
      "#2. tree\n",
      "#3. tree\n",
      "#4. tree\n",
      "#5. tree\n",
      "#6. tree\n",
      "#7. tree\n",
      "#8. tree\n",
      "#9. tree\n",
      "#10. tree\n",
      "#11. tree\n",
      "#12. tree\n",
      "#13. tree\n",
      "#14. tree\n",
      "#15. tree\n",
      "#16. tree\n",
      "#17. tree\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "rf = RandomForest(n_trees=20, n_sample=len(y_train), random_f=-1, max_depth=20)\n",
    "rf.fit(X_train, y_train, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9402948996857626\n"
     ]
    }
   ],
   "source": [
    "l = rf.accuracy(X_train, y_train)\n",
    "print('Accuracy: ', l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9352657004830918\n"
     ]
    }
   ],
   "source": [
    "# validation accuracy\n",
    "l = rf.accuracy(X_val, y_val)\n",
    "print('Accuracy: ', l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 1., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for submision\n",
    "y_pred = rf.predict(test_X_selected)\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "def results_to_csv(y_test):\n",
    "    y_test = y_test.astype(int)\n",
    "    df = pd.DataFrame({'Category': y_test})\n",
    "    df.index += 1  # Ensures that the index starts at 1. \n",
    "    df.to_csv('submission_spam.csv', index_label='Id')\n",
    "    \n",
    "results_to_csv(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
